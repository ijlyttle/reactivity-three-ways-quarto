[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data-Science Reactivity: Three Ways",
    "section": "",
    "text": "The purpose of this book is to compare and contrast reactive data-science apps using three languages/frameworks:\n\nR: Shiny\nPython: Dash\nJavaScript: Observable\n\nAn example app is created for each framework:\n\nstarting with the penguins data-frame from palmerpenguins:\n\nshow it as a table.\nspecify grouping columns, aggregation columns, and an aggregation function.\n\ncreate an aggregated data-frame from the input, using the specification.\n\nshow the aggregated data-frame as a table.\n\n\nThis book is written for folks who know how to develop basic Shiny apps, and wish to extend their knowledge to Dash (Python) or Observable (JavaScript). I will assume you have:\n\nbasic knowledge of how to build a Shiny app.\nsome familiarity with the tidyverse, in particular, dplyr.\n\nIn this book:\n\nwe’ll review a Shiny app, highlighting parts of its code.\nwe’ll look at a Dash app and the code.\nwe’ll look at an Observable Notebook, where the app is the code.\n\nThe goal is to give you the confidence to take the next steps to learn more about each of these frameworks.\n\n\nHere are some resources I have found useful:\n\nShiny’s tutorials.\nOnce you have built a few Shiny apps, it can be helpful to get a better sense of what makes Shiny “tick”. Joe Cheng gave an outstanding tutorial at the precursor to rstudio::conf() in 2016: Part 1, Part 2.\nHadley Wickham’s Mastering Shiny.\nAppsilon has a handy blog post: Dash vs. Shiny.\nDash’s documentation.\nFor an introduction to Observable, this tutorial page is a great start.\nIf you are comfortable with JavaScript and want to get a quick sense of Observable: the somewhat distractingly-named Observable’s not JavaScript."
  },
  {
    "objectID": "shiny.html",
    "href": "shiny.html",
    "title": "1  Shiny",
    "section": "",
    "text": "Shiny was my introduction to reactive programming. Like many folks, I started by hacking to “get stuff working”; this is a perfectly-honorable path. Then, I watched Joe Cheng’s tutorials (Part 1, Part 2), in which he explained some of the theory behind Shiny. These talks started me on a path that completely changed my persepective and, eventually, my abilities as a programmer.\nThis chapter is meant to be a review of Shiny; we will:"
  },
  {
    "objectID": "shiny.html#principles",
    "href": "shiny.html#principles",
    "title": "1  Shiny",
    "section": "1.1 Principles",
    "text": "1.1 Principles\nThese are some things to keep in mind to help you write more-understandable and predictable Shiny apps.\n\n1.1.1 Pure functions vs. side effects\nThis is the single biggest concept I have learned as a programmer, and I learned it relatively late in my career.\nA pure function has two properties:\n\ngiven the same set of arguments, it always returns the same value.\nit makes no changes outside of its scope.\n\nThis can provide us some big benefits:\n\nit doesn’t matter where or how the return value is computed, we can rely on getting the same answer.\nwe don’t have to worry about the environment changing as a result of calling the function.\n\nHere’s a couple of examples of pure functions:\n\nfunction(x) {\n  x**2 - 1\n}\n\nfunction(x) {\n  paste(x, \" using a pure function.\")\n}\n\nPure functions are relatively striaghtforward to test because the output depends only on the inputs.\nSide effects is a catch-all term for when a function’s behavior either:\n\ndepends on something not passed in as an argument.\nchanges the something outside of its scope, e.g.: writes a file, displays a plot.\n\nHere’s a couple of functions that either depend on or cause side effects:\n\n# return value depends on the *contents* of the file, not just file_name\nfunction(file_name) {\n  read.csv(file_name)\n}\n\n# this might make a change in a remote service\nfunction(url, data) {\n  \n  h <- curl::new_handle()\n  curl::handle_setform(h, data)\n  \n  curl::curl(url)\n}\n\nAside from being non-deterministic, functions with side effects can take a long time to execute.\nOf course, side effects are not necessarily bad things, but we need to be aware of them. Your Shiny server-function will make much more sense, and be much easier to debug, if you recognize pure functions and side effects.\n\n\n1.1.2 Reactives vs. observers\nShiny server-functions provide two broad mechanisms for updating the state of your app:\n\nreactive(): these return values, and work well with pure functions. In other words, the returned value depends only on the reactive values it depends on.\nobserve(): there is no return value; instead, these cause side-effects. Very often, the effect is to change something in the UI, such as the choices in an input, or to render a plot.\n\nIn Shiny, reactive expressions are designed to run quickly and often; observers are designed to be run sparingly.\n\n\n1.1.3 Using tidyverse functions\nThe tidyverse is designed with interactive programming in mind. It is meant to support code like this, without a lot of quotes or namespace qualifiers:\n\npenguins |>\n  group_by(island, sex) |>\n  summarise(bill_length_mm = mean(bill_length_mm))\n\nIn Shiny, variable (column) names in data frames are expressed as strings, rather than as bare variable-names. As well, in Shiny, we may want to summarise() an arbitrary set of variables. Thus, it can be a challenge to use tidyverse code in Shiny.\nIt should not surprise us that the tidyverse offers tools to address this situation:\n\n<tidy-select> is a set of tools to select variables within a data frame. Functions that use <tidy-select> include dplyr::select(), tidyr::pivot_longer(). Of particular use in Shiny are the selection helpers for strings: dplyr::any_of() and dplyr::all_of().\nacross() lets us use a <tidy-select> specification in a data-masking function. More concretely, it lets us group_by() or summarize() over an arbitrary set of variables in a data frame.\nIf you need to use data-masking with (by definition) a single variable, you can use subsetting with the .data pronoun, e.g. ggplot2::aes(x = .data[[str_var_x]])."
  },
  {
    "objectID": "shiny.html#demonstration-app",
    "href": "shiny.html#demonstration-app",
    "title": "1  Shiny",
    "section": "1.2 Demonstration App",
    "text": "1.2 Demonstration App\nThe goal of this chapter is to highlight some design choices in the source code of this demonstration Shiny app.\n\n1.2.1 Description\nTo start with, spend a few minutes playing with the app, while referring back to these diagrams:\n\n\n\nReactivity diagram for Shiny demo-app\n\n\nEach input and output you see in the diagram is a part of the UI of the app. The reactive expressions, in this case: inp and agg, are found only in the app’s server-function.\n\n\n\nLegend: Reactivity diagram for Shiny demo-app\n\n\nThe solid lines indicate immediate downstream-evaluation if the upstream value changes; this is what we think of when we hear “reactivity”. The dashed lines indicate that downstream-evaluation does not immediate follow an upstream change. For example, the reactive-expression agg is updated only when the button is pushed.\nSpend some time to study the app, to make sure that these diagrams agree with your understanding of how the app operates. In the following sections, we’ll discuss how to implement in your Shiny code.\n\n\n1.2.2 Prelims\nIn the rest of this chapter, we’ll highlight the code used to make app, and the design choices behind the code. In the repository, there are a couple of files to pay attention to:\napp-aggregate-local.R\nR/\n  aggregate-local.R\nHere’s the start of the app file, app-aggregate-local.R:\n\nlibrary(\"shiny\")\n\n# ------------------- \n# global functions\n# ------------------- \n#\n# created outside of reactive environment, making it easier:\n#   - to test\n#   - to migrate to a package\nsource(\"./R/aggregate-local.R\")\n\nAs you can see, it sources R/aggregate-local.R, which contains our helper functions.\n\n\n1.2.3 Helper functions\nBefore writing a Shiny app, I like to write out a set of non-reactive functions that will do the “heavy lifting”. To the extent possible, these are pure functions, which makes it easier to test. I keep these functions in an R folder alongside my app; here’s a link to the actual code.\nJust like in the app, we’ll use the palmerpenguins dataset:\n\n# this is not part of the helper functions - it's for exposition here\nlibrary(\"palmerpenguins\")\nlibrary(\"tibble\")\n\npenguins\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   <fct>   <fct>              <dbl>         <dbl>             <int>       <int>\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# … with 334 more rows, and 2 more variables: sex <fct>, year <int>\n\n\nIn fact, the first bit of code is not even a function. It is an enumeration of the choices for the aggregation function:\n\n# choices for aggregation functions\nagg_function_choices <- c(\"mean\", \"min\", \"max\")\n\nWe’ll use it in a few places, so I want to define it only once.\nNext, a couple of functions that, given a data frame, return the names of:\n\nnumerical variables\ncategorical variables\n\nYou might quibble with how I’ve defined these here, but it works for me, for this example.\n\n# given a data frame, return the names of numeric columns\ncols_number <- function(df) {\n  df_select <- dplyr::select(df, where(~is.numeric(.x) | is.integer(.x)) ) \n  names(df_select)\n}\n\n\n# given a data frame, return the names of string and factor columns\ncols_category <- function(df) {\n  df_select <- dplyr::select(df, where(~is.character(.x) | is.factor(.x)) ) \n  names(df_select)\n}\n\nYou may have noticed that I refer to functions using the package name, e.g. dplyr::select(). This is a habit I learned following Hadley Wickham; basically:\n\nI like to be as explicit as possible when writing functions. It provides fewer opportunities for strange things to happen; I provide enough opportunities as it is.\nThe function is more ready to be included in a package.\n\nAs advertised, testing (or at least spot-verification) is straightforward:\n\ncols_number(penguins)\n\n[1] \"bill_length_mm\"    \"bill_depth_mm\"     \"flipper_length_mm\"\n[4] \"body_mass_g\"       \"year\"             \n\n\n\ncols_category(penguins)\n\n[1] \"species\" \"island\"  \"sex\"    \n\n\nLet’s look at the aggregation function:\n\ngroup_aggregate <- function(df, str_group, str_agg, str_fn_agg, \n                            str_fn_choices = agg_function_choices) {\n  \n  # validate the aggregation function\n  stopifnot(\n    str_fn_agg %in% str_fn_choices\n  )\n  \n  # get the aggregation function\n  func <- get(str_fn_agg)\n  \n  df |>\n    dplyr::group_by(dplyr::across(dplyr::all_of(str_group))) |>\n    dplyr::summarise(\n      dplyr::across(dplyr::all_of(str_agg), func, na.rm = TRUE)\n    )\n}\n\nThere’s a few things I want to point out about this function:\n\nAside from the data frame, all the arguments are strings. It is designed for use with Shiny, not for interactive use.\nWe are using agg_function_choices to make sure that we won’t execute arbitrary code. We turn the string into binding to a function using get().\nWe use dplyr’s across() function, which lets us use select() semantics in “data-masking” functions, e.g. group_by(), summarise().\nTo select data-frame variables using strings, we use all_of().\n\nFor example if we were grouping by \"island\", then aggregating over \"bill_length_mm\" and \"bill_depth_mm\" using \"mean\", our interactive code might look like:\n\nlibrary(\"dplyr\", quietly = TRUE)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\naggregate_interactive <- \n  penguins |>\n  group_by(island) |>\n  summarise(\n    bill_length_mm = mean(bill_length_mm, na.rm = TRUE),\n    bill_depth_mm = mean(bill_depth_mm, na.rm = TRUE)\n  )\n\naggregate_interactive\n\n# A tibble: 3 × 3\n  island    bill_length_mm bill_depth_mm\n  <fct>              <dbl>         <dbl>\n1 Biscoe              45.3          15.9\n2 Dream               44.2          18.3\n3 Torgersen           39.0          18.4\n\n\nWe can use this result to help verify that our “string” version is working:\n\naggregate_string <- group_aggregate(\n  penguins, \n  str_group = \"island\", \n  str_agg = c(\"bill_length_mm\", \"bill_depth_mm\"),\n  str_fn_agg = \"mean\"\n)\n\nidentical(aggregate_interactive, aggregate_string)\n\n[1] TRUE\n\n\n\n\n1.2.4 UI\nThe UI object is relatively straightforward; we use a fluidPage() with a narrower column for inputs and a wider column for outputs.\nTo give a clearer view of the high-level structure of the page, I replaced the code for the inputs and outputs with ...:\n\nlibrary(\"shiny\")\n\nui <- fluidPage(\n  titlePanel(\"Aggregator\"),\n  fluidRow(\n    column(\n      width = 4, \n      wellPanel(\n        h3(\"Aggregation\"),\n        ...\n      )s\n    ),\n    column(\n      width = 8,\n      h3(\"Input data\"),\n      ...\n      hr(),\n      h3(\"Aggregated data\"),\n      ...\n    )\n  )\n)\n\n\n1.2.4.1 Inputs\n\nwellPanel(\n  h3(\"Aggregation\"),\n  selectizeInput(\n    inputId = \"cols_group\",\n    label = \"Grouping columns\",\n    choices = c(),\n    multiple = TRUE\n  ),        \n  selectizeInput(\n    inputId = \"cols_agg\",\n    label = \"Aggregation columns\",\n    choices = c(),\n    multiple = TRUE\n  ),\n  selectizeInput(\n    inputId = \"func_agg\",\n    label = \"Aggregation function\",\n    choices = agg_function_choices,\n    multiple = FALSE\n  ),\n  actionButton(\n    inputId = \"button\",\n    label = \"Submit\"\n  )\n)\n\nLet’s look more closely at input$cols_group (this also applies to input$cols_agg):\n\nselectizeInput(\n  inputId = \"cols_group\",\n  label = \"Grouping columns\",\n  choices = c(),\n  multiple = TRUE\n)\n\nNote that choices is specified, initially, as an empty vector. The reactivity diagram for cols_group indicates that, we use an observer function to update this input. We’ll do this in the server function, where we update the choices.\n\n\n1.2.4.2 Outputs\nThe outputs are fairly strightforward; we are using DT::DTOutput() as placeholders for DT DataTables.\n\ncolumn(\n  width = 8,\n  h3(\"Input data\"),\n  DT::DTOutput(\n    outputId = \"table_inp\"\n  ),\n  hr(),\n  h3(\"Aggregated data\"),\n  DT::DTOutput(\n    outputId = \"table_agg\"\n  )      \n)\n\n\n\n\n1.2.5 Server function\nThis may be a habit particular to me, but I like to organize a server-function into groups:\n\nserver <- function(input, output, session) {\n  # input observers\n  # reactive expressions and values\n  # outputs\n}  \n\n\n1.2.5.1 Input observers\nThere are two inputs: cols_group and cols_agg, whose choices change when the input data-frame changes.\nTo make such a change, we use a Shiny observe(), which runs when any of its reactive dependencies change. An observe() does not return a value; instead, it causes a side-effect. In this case, it changes an input element in the DOM.\nThe observers are substantially similar, so I’ll show only cols_group:\n\nobserve({\n  # this runs whenever the parsed input data changes\n  updateSelectizeInput(\n    session,\n    inputId = \"cols_group\",\n    choices = cols_category(inp())\n  )\n}) \n\nNote that one of our helper functions, cols_category(), makes an appearance. The choices for the cols_group input are updated according to the names of the categorical variables in the data frame returned by inp().\n\n\n1.2.5.2 Reactive expressions\nThis app uses two reactive expressions:\n\ninp(), which returns the input data-frame.\nagg(), which returns the aggregated data-frame.\n\n\ninp <- \n  reactive({ \n    palmerpenguins::penguins\n  }) \n\nFor this app, we probably did not need to wrap palmerpenguins::penguins in a reactive(). I did this with future expansion in mind, where inp() could also return a data frame according to a choice, or even a data frame parsed from an uploaded CSV file.\nThe reactive expression for agg(), the aggregated data-frame, is more interesting:\n\nagg <- \n  reactive({\n         \n    req(input$func_agg %in% agg_function_choices)\n \n    group_aggregate(\n      inp(), \n      str_group = input$cols_group, \n      str_agg = input$cols_agg, \n      str_fn_agg = input$func_agg\n    )\n  }) |>\n  bindEvent(input$button, ignoreNULL = TRUE, ignoreInit = TRUE)\n\nThe first thing we do in the reactive is make sure that the value of input$func_agg is among the choices we specified. I’m sure you noticed that this is an extra check. Although redundant, I am careful to validate using the same values: agg_function_choices. You can read more about input validation in the security chapter of Mastering Shiny.\nThen, we use our group_aggregate() helper function. For me, having tested it outside of Shiny helped me focus on getting the rest of the code working.\nThe reactive() expression returns the data; the expression itself is piped to bindEvent(), which will run the reactive(), and return its value, only when the value of input$button changes. This is a relatively new pattern in Shiny; it appeared in v1.6.0.\nbindEvent() has a couple of options:\n\nignoreNULL = FALSE: the reactive() is not evaluated if input$button is zero.\nignoreInit = FALSE: the reactive() is not evaluated when the app is first initialized.\n\nIn this case, the reactive() is evaluated only in response to a button-click. This can be a useful pattern if the reactive() contains a long-running computation, or a call to an external resource. You may also be interested in Shiny’s bindCache() function.\n\n\n1.2.5.3 Outputs\nThere two outputs: one for the inp() data, the other for the agg() data; each is a table output.\nThese outputs are similar to one another; we’ll focus on output$table_inp:\n\noutput$table_inp <- DT::renderDT(inp())\n\nThe table output is a straightforward use of DT::renderDT()."
  },
  {
    "objectID": "dash.html",
    "href": "dash.html",
    "title": "2  Dash",
    "section": "",
    "text": "Although it is a gross oversimplification, at first glance Dash seems like Python’s answer to Shiny.\nAccordingly, the goals of this chapter are:\nTo provide some context, we will use this demonstration app, and examine its code."
  },
  {
    "objectID": "dash.html#principles",
    "href": "dash.html#principles",
    "title": "2  Dash",
    "section": "2.1 Principles",
    "text": "2.1 Principles\nBoth Shiny and Dash use the idea of a reactive graph, which indicates what things depend on what other things:\n\nIn Shiny, the reactive graph (what depends on what) is inferred using the code in reactive expressions.\nIn Dash, it is explicit, which is a mixed blessing.\n\nFor Dash, this explicitness provides the flexibility to do a lot of things, but the price is that you have to specify:\n\nall the DOM components (UI elements) in the layout.\neach connection between components is governed by a callback function that you provide.\n\nThis is substantially similar to Shiny: part of the Dash app is used to define the UI; the rest defines what happens on the server.\nHowever, there are a couple of big considerations:\n\nthe state of the app cannot be stored in the server application.\nit is easiest to move you data around using JSON and base64-encoded data.\n\nThis is different from Shiny, which stores the state of the application in the server function. Further, Shiny manages the serialization/de-serialization of data to/from the UI; with Dash, you have to manage that yourself.\nThese are not insurmountable obstacles, as well see in the rest of this section. For example, one place to store the state is the user’s browser, in the web-page (DOM) itself.\n\n2.1.1 Everything that exists is a component\nThese first two subsections are an homage to the famous John Chambers quote:\n\nTo understand computations in R, two slogans are helpful:\n\nEverything that exists is an object.\nEverything that happens is a function call.\n\n\nSimilarly, everything that exists on Dash app’s web page is a component.\nAs we’ll see in the demo, a Dash app contains a layout that you specify:\n\napp.layout = html.div(...)\n\nYou need to fill in the .... A component might be a straightforward HTML element, or it might be a Dash component, where you define the attributes.\nThe html object (imported from the dash package) behaves very similarly to R’s htmltools package; they are both based on the HTML5 spec.\nWe’ll see more in the demo, but here’s an example of a Dash component:\n\ndcc.Dropdown(id='cols-group', multi=True)\n\nThis is a dropdown component; we define the id and multi properties at definition. In this case, we don’t define the options or value properties. We’ll update the options dynamically, and let the user set the value.\nLike other components, dropdowns have a number of properties; we can set them either at initialization, as we did here, or we can set them using a callback.\n\n\n2.1.2 Everything that happens is a callback\nIf you want something to happen in a Dash app, it has to happen in a callback function. Dash lets you write callbacks using Python. It also lets you write callbacks in JavaScript, but that gets beyond the scope of this book.\nWe’ll see this in more detail in the demo app, but a callback is a standard Python function with a decorator:\n\n@app.callback(Output('cols-group', 'options'),\n              Input('inp', 'data'))\ndef update_cols_group(data_records):\n    return cols_choice(data_records, 'object')\n\nThe decorator, @app.callback(...) tells Dash which layout components to map to the function’s inputs and outputs. When an Input() changes:\n\nthe browser calls the Dash server to run the callback function.\nthe Dash server runs the Python function.\nthe Dash server sends the Output() to the browser.\nthe browser updates the DOM.\n\n\n\n2.1.3 Server cannot store state\nManaging state is a pain. However, by remaining stateless, Dash is able to easily scale to as many server instances it needs because it does not matter which instance of a callback-function responds to which browser (user) making the call.\nComing from Shiny, this might seem like a show-stopper; we are used to manipulating, then storing data using the server side of an app. But there are ways around this. It’s not that you can’t store the state - you just can’t store it “here”. Your options are:\n\nstore data in the DOM, then send it when needed.\nstore data in an external database, or the like.\n\nWe’ll use the first option here. Here’s one of the components in our layout:\n\ndcc.Store(id='inp', data=penguins.to_dict('records'))\n\nNote that this component is initialized using the penguins data, but that we are using pandas’ to_dict() method. This is because the component will receive the data using JSON; it is stored in the DOM as a JavaScript object.\n\n\n2.1.4 Use JSON or base64\nThe final thing to keep in mind is that when we communicate data between the browser DOM and the callback functions, it does not use native Python objects. Instead, from the Python callback-functions’ perspective, data is serialized to JSON when sent to the DOM, and deserialized from JSON when received from the DOM.\nFor Python dictionaries and lists containing numbers and strings, the serialization process is implied.\nThere are (at least) a couple of conventions for serializing a data frame to JSON: by row or by column.\nComing from R, you may think of a data frame as a list of vectors, each of the same length. This is column-based, for example:\n{\n  \"species\": [\"Adelie\", \"Adelie\"],\n  \"bill_length_mm\": [39.1, 18.7]\n}\nAlternatively, the row-based approach:\n[\n  {\"species\": \"Adelie\", \"bill_length_mm\": 39.1},\n  {\"species\": \"Adelie\", \"bill_length_mm\": 18.7}\n]\nThe row-based approach seems to be the convention in Dash; this is the approach used by D3. Here, we think of a data frame as a collection of records. To serialize from Pandas, we’ll use to_dict('records'); to deserialize (import) into Pandas, we’ll use from_dict().\nIn the context of the Python code, I’ll refer to data formats as either:\n\ndata-frame format, i.e. Pandas data frame\nrecords format, i.e. JSON collection of records\n\nThe other option is to use base64 encoding; I have seen this used for uploading/downloading text files, e.g. CSV or JSON files."
  },
  {
    "objectID": "dash.html#demonstration-app",
    "href": "dash.html#demonstration-app",
    "title": "2  Dash",
    "section": "2.2 Demonstration app",
    "text": "2.2 Demonstration app\nI’ve tried to outline, briefly, some of the principles used to build a Dash app; I think they will make more sense in the context of the Dash demo-app.\n\n2.2.1 Description\nHere is the reactive graph for the demonstration app.\nIt’s a little busier than the Shiny app; Dash forces the developer to be explicit, Shiny makes some things implicit.\n\n\n\nReactivity diagram for Dash demo-app\n\n\nA few things to note about this diagram:\n\nEach rectangle is a layout component; each circle is a callback function.\nComponents have properties, which are associated with arguments to callback functions.\nWhen an argument to a callback function changes, the function is (re-)run.\nThe output(s) of callback functions are used to update the properties of components.\nFunctions are run, properties are updated, and so on, until the app “comes to rest”.\n\nThere are a few more things formalized in the diagram, but we’ll get to them as we discuss the demo app:\n\n\n\nLegend: Reactivity diagram for Dash demo-app\n\n\n\n\n2.2.2 Prelims\nLike we did with Shiny, in the rest of this chapter, we’ll highlight the code used to make app, and the design choices behind the code. In the repository, there are a some files to pay attention to:\napp-aggregate-local.py\nhelpers/\n  __init__.py\n  aggregate.py\n  cols.py\nHere’s the app file, app-aggregate-local.py, with the components and callbacks removed:\n\n# Run this app with `python app-aggregate-local.py` and\n# visit http://127.0.0.1:8050/ in your web browser.\n\nimport dash\nfrom dash.dependencies import Input, Output, State\nfrom dash import dash_table\nfrom dash import html\nfrom dash import dcc\nimport dash_bootstrap_components as dbc\n\nimport pandas as pd\nfrom palmerpenguins import load_penguins\n\nfrom helpers.aggregate import aggregate_df, agg_function_choices\nfrom helpers.cols import cols_choice, cols_header\n\npenguins = load_penguins()\n\napp = dash.Dash(\n    __name__, \n    external_stylesheets=[dbc.themes.BOOTSTRAP]\n)\n\n# make `server` available to Heroku\nserver = app.server\n\n# <component layout>\n\n# <callback functions>\n\nif __name__ == '__main__':\n    app.run_server(debug=True)\n\nThe first part of the file handles the imports. You will need to have installed (hopefully in your virtual environment): dash, dash_bootstrap_components, pandas, and palmerpenguins. For example:\n> pip install dash\nThe remaining bits of the file set up our app:\n\nLoad the penguins data.\nCreate the app. This is the standard way to create a Dash app; note that we are using the Bootstrap styling.\nWe make the app.server available at the top level as server. This goes a little beyond the scope of this chapter, but we are doing this as a part of deploying the app on Heroku.\nComponents and callbacks, we’ll look at these in detail in following sections.\nA standard bit of code that tells Python to app.runServer() when we execute this file.\n\nWe also have a helpers directory with some files. The __init__.py file is empty; its purpose is to signal to Python that there are .py files in this directory with functions that can be imported.\n\n\n2.2.3 Helper functions\nJust like in R, I like to write as much “vanilla” Python code as I can. I want to demonstrate to myself, as much as I can, that the “guts” of the app behaves as I expect. That way, when I’m building the components and the callbacks, I can narrow down the things that might be going wrong.\n\n# using only to demonstrate code; not used in Python Dash.\nlibrary(\"reticulate\")\nuse_virtualenv(\"./venv\")\n\nI’m reproducing this code in an Quarto document using the R engine, so I’m using the reticulate package to run Python. You will not need to do this to create a Dash app; you can work in a purely Python environment.\nIn fact, if you are newish to Python, you might find the “Field Guide” appendix useful to get things set up. For Python programming, there is a great introduction at reticulate.\nOur first step is to get a look at the penguins dataset, to verify it’s the same as we use in R.\n\n# importing into RMarkdown environment for exposition\nimport pandas as pd\nfrom palmerpenguins import load_penguins\n\npenguins = load_penguins()\nprint(penguins)\n\n       species     island  bill_length_mm  ...  body_mass_g     sex  year\n0       Adelie  Torgersen            39.1  ...       3750.0    male  2007\n1       Adelie  Torgersen            39.5  ...       3800.0  female  2007\n2       Adelie  Torgersen            40.3  ...       3250.0  female  2007\n3       Adelie  Torgersen             NaN  ...          NaN     NaN  2007\n4       Adelie  Torgersen            36.7  ...       3450.0  female  2007\n..         ...        ...             ...  ...          ...     ...   ...\n339  Chinstrap      Dream            55.8  ...       4000.0    male  2009\n340  Chinstrap      Dream            43.5  ...       3400.0  female  2009\n341  Chinstrap      Dream            49.6  ...       3775.0    male  2009\n342  Chinstrap      Dream            50.8  ...       4100.0    male  2009\n343  Chinstrap      Dream            50.2  ...       3775.0  female  2009\n\n[344 rows x 8 columns]\n\n\nBecause we will tell Dash to move data back and forth using a dictionary, we’ll make a variable that stores penguins as records:\n\npenguins_records = penguins.to_dict('records')\nprint(penguins_records[0:2]) # just to get a flavor\n\n[{'species': 'Adelie', 'island': 'Torgersen', 'bill_length_mm': 39.1, 'bill_depth_mm': 18.7, 'flipper_length_mm': 181.0, 'body_mass_g': 3750.0, 'sex': 'male', 'year': 2007}, {'species': 'Adelie', 'island': 'Torgersen', 'bill_length_mm': 39.5, 'bill_depth_mm': 17.4, 'flipper_length_mm': 186.0, 'body_mass_g': 3800.0, 'sex': 'female', 'year': 2007}]\n\n\nThe first function we’ll verify is used to tell us, for a given a data frame and a given Pandas type, which columns are of that type. (In the app, we’ll convert to records-format in the callback function.)\n\ndef cols_choice (df, include):\n\n    return df.select_dtypes(include=include).columns.to_list()\n\nLet’s test the function. In Pandas, strings have type 'object':\n\ncols_choice(penguins, 'object')\n\n['species', 'island', 'sex']\n\n\nNumbers have type 'number':\n\ncols_choice(penguins, 'number')\n\n['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g', 'year']\n\n\nSo far, so good.\nNext, we need a function to generate the (display) table properties. We need to return a list where each element is dictionary that describes a column in the data frame. As we’ll see later, this is the format we need to specify the table headers.\nNote that we are not using the data-frame format in this function, so we can keep everything in records format. We use a trick by looking at only the first (well, zeroth) record.\n\ndef cols_header (data_records):\n  \n    if (len(data_records) == 0):\n        return []\n\n    return [{'name': v, 'id': v} for v in data_records[0].keys()]\n\nThere’s all sorts of ways to do this in Python. I’m a sucker for functional programming, so here’s how to use a map() function. This returns a map object, so we need to change it to a list.\n\nlist(\n    map(lambda v: {'name': v, 'id': v}, data_records[0].keys())\n)\n\nThis works largely like R’s purrr::map(), but in Python, we provide the function first, then the thing we are iterating over. Here’s a purrr equivalent:\n\npurrr::map(names(data_records[1]), ~list(name = .x, id = .x))\n\nAnyhow, let’s try out our Python function:\n\ncols_header(penguins_records)  \n\n[{'name': 'species', 'id': 'species'}, {'name': 'island', 'id': 'island'}, {'name': 'bill_length_mm', 'id': 'bill_length_mm'}, {'name': 'bill_depth_mm', 'id': 'bill_depth_mm'}, {'name': 'flipper_length_mm', 'id': 'flipper_length_mm'}, {'name': 'body_mass_g', 'id': 'body_mass_g'}, {'name': 'sex', 'id': 'sex'}, {'name': 'year', 'id': 'year'}]\n\n\nThis seems OK.\nFinally, we need an aggregation function. We send a data frame, then some lists of column-names, and a string naming an aggregation-function. This is an exercise in Pandas - I think this works, but my lack of experience in Pandas suggests there may be a “better” way. We’ll write this as in terms of data frames; we’ll convert to/from records-format in the callback function.\n\nagg_function_choices = ['mean', 'min', 'max']\n\ndef aggregate_df (df, cols_group, cols_agg, func_agg,\n                  str_fn_choices = agg_function_choices):\n    \n    if not func_agg in str_fn_choices:\n        raise AssertionError(f\"{func_agg} not a legal function-choice\")\n  \n    if (cols_group != None):\n        df = df.groupby(cols_group)\n\n    if (cols_agg == None or len(cols_agg) == 0):\n        return []\n    \n    # dictionary, keys: column-names, values: function-name\n    dict_agg = {i: func_agg for i in cols_agg}\n    \n    df = df.agg(dict_agg).reset_index()\n\n    return df\n  \naggregate_df(\n  penguins, \n  cols_group = \"island\", \n  cols_agg = [\"bill_length_mm\", \"bill_depth_mm\"],\n  func_agg = \"mean\"\n)  \n\n      island  bill_length_mm  bill_depth_mm\n0     Biscoe       45.257485      15.874850\n1      Dream       44.167742      18.344355\n2  Torgersen       38.950980      18.429412\n\n\nThis may be interesting only to me, but there is a more “functional” way to write the statement:\n\ndict_agg = {i: func_agg for i in cols_agg}\n\nUsing a reducer:\n\nfrom functools import reduce\n\ndict_agg = reduce(lambda acc, val: dict(acc, **{val: func_agg}), cols_agg, {})\n\nIn this situation, it’s more verbose and, I think, less clear what is going on unless you are into functional programming. Please pardon this diversion.\nConfident that the “guts” of the app works, we move onto the components and the callbacks.\n\n\n2.2.4 Compmonent layout\nLet’s start with the formatting. In the app, we need to create an app.layout, which will be the HTML used to render our app in the browser. Just like with Shiny, we’re using Bootstrap, but we have to be a little more explicit. In Shiny, Bootstrap is “baked in”; in Dash, it’s an add-on.\nHere’s the “formatting” bits of the layout; the ... represent Dash components that we’ll discuss presently:\n\napp.layout = html.Div(\n    className='container-fluid',\n    children=[\n        ...\n        html.H2('Aggregator'),\n        html.Div(\n            className='row',\n            children =[\n                html.Div(\n                    className='col-sm-4',\n                    children=[\n                        dbc.Card([\n                            dbc.CardHeader('Aggregation'),\n                            dbc.CardBody([\n                              ...\n                            ])\n                        ])\n                    ]\n                ),\n                html.Div(\n                    className='col-sm-8',\n                    children=[\n                        html.H3('Input data'),\n                        ...\n                        html.Hr(),\n                        html.H3('Aggregated data'),\n                        ...\n                    ]\n                )\n            ]\n        )\n    ]\n) \n\n\n2.2.4.1 Data stores\nThe first parts I want to highlight are the data stores:\n\ndcc.Store(id='inp', data=penguins.to_dict('records')),\ndcc.Store(id='agg', data=[])\n\nRecall that the server part of a Dash app cannot store its state. Instead, we will store the state, in our case: the input and aggregated data, in the DOM. Dash provides a dcc.Store() component; in its data property, we store the records-based data (note the use of .to_dict('records'). Also note that, just like Shiny, components each have an id property.\n\n\n2.2.4.2 Inputs\nLet’s look at the components that specify the aggregation:\n\ndcc.Dropdown(id='cols-group', multi=True),\ndcc.Dropdown(id='cols-agg', multi=True),\ndbc.Select(\n    id='func-agg',\n    options=[{'label': v, 'value': v} for v in agg_function_choices],\n    value=agg_function_choices[0]\n),\ndbc.Button(id='button-agg', children='Submit', class_name='btn btn-secondary')\n\nHere, we’re using a couple of “standard” (dcc) dropdowns and a couple of Bootstrap (dbc) components.\nWith the dropdowns, other than providing the id, we specify that multiple selections can be made. We don’t populate the options; we’ll do this using a callback. If we were only ever going to use one input-dataset, in this case penguins, it might make sense to populate the options when defining the component. Populating using a callback function makes our code more general, allowing the case where we could upload an abitrary dataset. Perhaps it wasn’t necessary to follow the more-general approach here, but it allowed me to learn more about how Dash works.\nWe define our dbc.Select() component completely in the layout.\nNote also that we can apply Bootstrap classes todbc.Button(). What we might think of as a label is specified as the children of the button element.\n\n\n2.2.4.3 Outputs\nIt remains to look at the data tables. There are two tables, identical in form; we will examine only one of them:\n\ndash_table.DataTable(\n    id='table-inp',\n    page_size=10,\n    sort_action='native'\n)\n\nThe properties here are straightforward: we need an id, we want to show ten entries at a time, sort_action='native' indicates that we want sorting to be available (and for Dash to take care of it).\nWe will populate the data tables using callback functions.\n\n\n\n2.2.5 Callback functions\nIn Dash, callback functions are just regular functions with decorators. Here’s a very generic example:\n\n@app.callback(Output('output-component', 'output-property'),\n              Input('input-component', 'input-property'))\ndef some_function_name(x):\n    y = some_function(x)\n    \n    return y\n\nThe decorator tells Dash how to build a function that wraps the “actual” function. We are not concerned with the implementation; we are interested in the interface.\nThe decorator is this bit of code: @app.callback(). It takes a series of arguments which map to the function’s parameters and return values:\n\nOutput('output-component', 'output-property') is mapped to the return value.\nInput('input-component', 'input-property') is mapped to the x parameter.\n\nThis tells Dash that whenever the input-property of the input-component changes, it should:\n\nrun the function using the input-property as an argument, then\nsend the return value to the output-property of the output-component\n\nWe’ll see some more-complex cases in the following examples.\n\n2.2.5.1 Inputs\nWe have a couple of input components that need updating: the grouping columns and the aggregation columns. Each has its own callback, but they are virtually identical, so I’ll describe only one.\n\n@app.callback(Output('cols-group', 'options'),\n              Input('inp', 'data'))\ndef update_cols_group(data_records):\n    df = pd.DataFrame.from_dict(data_records)\n    return cols_choice(df, 'object')\n\nWhenever the inp data changes:\n\nthe function is called using the input data (in records form).\nthe function converts to data-frame format, then\nuses our helper function to determine the columns that have string values.\nit returns a list of column names, which Dash uses to update the cols-group options.\n\n\n\n2.2.5.2 Calculations\nThe callback that performs the aggregation has more going on, but we’ll get to the bottom of it.\n\n@app.callback(Output('agg', 'data'),\n              Input('button-agg', 'n_clicks'),\n              State('inp', 'data'),\n              State('cols-group', 'value'),\n              State('cols-agg', 'value'),\n              State('func-agg', 'value'),\n              prevent_initial_call=True)\ndef aggregate(n_clicks, data_records, cols_group, cols_agg, func_agg):\n    # create DataFrame\n    df = pd.DataFrame.from_dict(data_records)\n\n    # aggregate\n    df_new = aggregate_df(df, cols_group, cols_agg, func_agg)\n\n    # serialize DataFrame\n    return df_new.to_dict('records')\n\nA couple of things to sort through:\n\nThe function has five parameters; the decorator has one Input() and four instances of State().\nThe decorator is provided an additional argument: prevent_initial_call.\n\nA State() is similar to an Input(); the difference that the function is not run in response to a State() change. In this case, the function is run only in response to the value of the button changing, i.e. being clicked.\nThe prevent_inital_call argument describes itself well. By setting it to True, we ensure that the only way the aggregation function will be run is when the button is clicked.\nFinally note that we convert our input records into a data frame, then use records-format for the return value.\n\n\n2.2.5.3 Outputs\nThe final set of callbacks is for our data tables. Again, although we have two data tables, the callbacks are virtually identical, so I’ll highlight only one.\n\n@app.callback(Output('table-inp', 'columns'),\n              Output('table-inp', 'data'),\n              Input('inp', 'data'))\ndef update_table_inp(data_records):\n     return cols_header(data_records), data_records  \n\nThe thing to note there is that Dash and Python support multiple return-values, mapping each to an Output().\nThis situation merits multiple return-values because a Dash data table has a properties for data and columns (headers). This information comes from the same source, so it made sense to use multiple return-values.\nCertainly, you could write two callbacks to do the same things; writing a single callback made more sense to me. Of course, you should do what makes sense to you."
  },
  {
    "objectID": "observable.html",
    "href": "observable.html",
    "title": "3  Observable",
    "section": "",
    "text": "Compared with Shiny and Dash, Observable seems like another world:\nThat said, there’s a few things from your R and tidyverse world that may help you get acquainted:\nI try to keep in mind that the point of the exercise, largely, is to “do stuff to data frames”. Knowing how to “do stuff” and “think about stuff” using tidyverse makes it easier for me to figure out the same “stuff” elsewhere."
  },
  {
    "objectID": "observable.html#principles",
    "href": "observable.html#principles",
    "title": "3  Observable",
    "section": "3.1 Principles",
    "text": "3.1 Principles\n\n3.1.1 Hosted service runs in browser\nThe best-known use for Observable is at the site for which is is named: Observable.\nLike many hosted services, the Observable website is free to use if everything you are doing is open, i.e. the GitHub model.\nThe Observable service uses the Observable runtime and the Observable standard-library; these are also available in the new Quarto platform developed by RStudio.\n\n\n3.1.2 Reactivity baked in\nGenerally, each cell in an Observable notebook returns a value that is bound to a variable. Here’s a straightforward example:\na = 3\nb = a\nIf we change the first cell, such that a = 4, the value of b is automatically updated; we don’t need to run any other cells.\nAlthough an Observable notebook appears like a Jupyter notebook, or like an RMarkdown document, there are some important differences:\n\nAs mentioned above, values of cells are updated automatically, much like an Excel spreadsheet.\nOne consequence of this is that notebooks need not follow a linear order from top to bottom. A value set later in the notebook can be referenced earlier in the notebook.\nJust about everything you do in an Observable notebook is in a cell. A cell can be JavaScript, Markdown, HTML, TeX, or SQL. (The vast majority of cells in my notebooks are Markdown or JavaScript.)\n\n\n\n3.1.3 JavaScript\nAlthough Observable cells can use a variety of languages, the core language is JavaScript. Or at least a close approximation to JavaScript.\nComing from R, these are the biggest things I need to keep in mind:\n\nObjects (analgous to R’s named lists) and arrays (analgous to R’s unnamed lists and vectors) are mutable. If you pass an object as an argument to a function, then change the object in the function, the original object is changed. This differs from R, and can lead to nasty surprises.\nStrings and numbers are immutable. Also, a scalar value is different from an array containing a single scalar value.\n\n\n\n3.1.4 Tidyverse thinking helps\nIt does take a while to get used to JavaScript. That said, it is more-and-more becoming a language for data-science alongside R and Python.\nPersonally, I rely on the mental models I have developed using dplyr, purrr, tidyr, and ggplot2. When working in JavaScript, there may or may not be an analogue to the tidyverse function you have in mind. The JavaScript function may take arguments in a different order, or have a completely different way of working. For me, it helps to know “what I want to do with the data”. It also helps to have the confidence of having done something similar using tidyverse.\n\n\n3.1.5 viewof is a useful construct\nThis is something particular to Observable, not JavaScript in general. Once I started to get comfortable with viewof, Observable got easier for me.\nWe’ll see this pattern used many times in the example, but it may be useful to Consider an Observable input (not operable in this book):\n\n\n\n\n\nObservable button\n\n\n\n\nviewof clicks = Inputs.button(\"OK\", {label: \"Click me\"})\nIn this context, the variable clicks:\n\nhas a value: number of times the button has been clicked.\nhas a view: the rendered view in the browser.\n\nWhen we use viewof clicks = ..., we are telling Observable:\n\nwe want to view the button here\nwe want to bind the value of the button to the variable clicks\n\nWe can use the variable clicks elsewhere in the notebook.\nThe view is a side-effect; the value is, well, a value."
  },
  {
    "objectID": "observable.html#demonstration-app",
    "href": "observable.html#demonstration-app",
    "title": "3  Observable",
    "section": "3.2 Demonstration app",
    "text": "3.2 Demonstration app\nHere’s the link to the now-familar aggregator app.\nIn Observable, there is not a clear distiction between an input and an output. I find it helpful to think of everything in Observable as a reactive varriable.\n\n\n\nReactivity diagram for Observable demo-app\n\n\nAs noted above, and as we’ll see in greater detail, we use the viewof interface often to display things to the screen, while keeping track of the value. This is such an important concept that I indicate which of the variables in the app use the viewof interface.\n\n\n\nLegend: Reactivity diagram for Observable demo-app\n\n\nObservable does not require variables to be defined in any particular order. As a result, I have adapted a style (I’ve see others do it, too) where a notebook has three sections:\n\nShowcase: mostly graphical and/or interactive, aimed at a general audience.\nWorkshop: contains supporting code and explanations, aimed at a more-technical audience.\nAppendix: import objects and offer functions for other notebooks to import.\n\nIn this chapter, we’ll go over this “backwards”.\n\n3.2.1 Appendix\nHere’s where we import stuff into our notebook.\nimport { aq, op } from \"@uwdata/arquero\"\nHere, we’re importing objects from another notebook, in this case, a notebook that features the arquero library.\nArquero contains functionality along the lines of dplyr and tidyr.\nAlso tidyjs does much the same thing - it’s a matter of preference which you use. Tidyjs is designed to be familiar to tidyverse users.\nI use a lot of Vega-Lite; arquero is made by the same group. Also, arquero is designed to work with Apache Arrow.\n\n\n3.2.2 Workshop\nOur first step is to import our data into the notebook. One way to do that is to use a file attachment, one of the few times we interact with Observable not using a cell.\nIf we have the result of a multi-step process that we want to put into a variable, we can make put the code in some curly braces, then return the result:\ninp = {\n  const text = await FileAttachment(\"penguins.csv\").text();\n  const textRemoveNA = text.replaceAll(/,NA/gi, \",\");\n\n  return aq.fromCSV(textRemoveNA);\n}\nHere, we see that we import the text, then remove instances of \"NA\". This puts the text in a format that can be parsed by arquero.fromCSV(), which returns an arquero Table.\nThe notebook is designed such that we can bind inp to any arquero Table, not just penguins, and it should work equally well.\nNext, we need a function to help us determine which columns can be used for grouping, and which for aggregation.\nThis is a personal habit since trying to be more aware of functional programming, but whenever I make a function in Observable, I like to make the signature as prominent as possible. I use a variation of Hindley-Miller notation, which is a fancy way of saying that I want to keep track of the types for the parameters and return-value:\n/* (Table, (* -> Boolean)) -> [String]\n *\n * Given an arquero table and a predicate-function,\n * return an array of strings corresponding to names of\n * columns that satisfy the predicate.\n *\n * This can be useful to identify which columns are strings\n * or numbers, etc.\n *\n * Note that null values are removed before the predicate\n * is applied.\n */\ncolumnNamesPredicate = function (data, predicate) {\n  const colNames = data.columnNames();\n  const keep = colNames.filter((x) =>\n    data\n      .array(x)\n      .filter((x) => !_.isNull(x))\n      .every(predicate)\n  );\n  return keep;\n}\nNote that the second parameter, predicate, is a function that takes any type of value and returns a boolean. If I wanted to return the names of string-columns, I would supply the Lodash function _.isString.\nAn arquero table is a object of arrays, just like R’s data frame is a list of (most-often) vectors; it’s a column-based approach.\nFirst, we get an array of colNames. Then we filter this array using another predicate function:\n\ndata.array(x): given the array of values in the column named x,\n.filter((x) => !_.isNull(x)): keep only those values that are not null,\n.every(predicate): return true if every value in the array satisfies the predicate function we supply.\n\nWe return only those column names where our predicate function returns true.\nWe also need a function to build an arquero query-object based on our specification.\n/* ([String], [String], String) -> Object\n *\n * Given an array of column names for grouping, an array of\n * column names for aggregations, and the name of an aggregation\n * function, return an object used to construct an Arquero query.\n *\n * The query will group by `cols_group`, then rollup (aggregate)\n * over `cols_agg`, using the function identified using `func_agg`.\n */\nbuildQueryObject = function (cols_group, cols_agg, func_agg) {\n  const values = cols_agg.reduce(\n    (acc, val) => ({\n      ...acc,\n      [val]: { expr: `(d) => op.${func_agg}(d[\"${val}\"])`, func: true }\n    }),\n    {}\n  );\n\n  const queryObject = {\n    verbs: [\n      { verb: \"groupby\", keys: cols_group },\n      { verb: \"rollup\", values: values }\n    ]\n  };\n\n  return queryObject;\n}\nThere are two operations in this query:\n\n\"groupby\", where we use the cols_group.\n\"rollup\", where we build another object to specify the aggregation.\n\nIf our aggregation function is min, and our aggregtion columns are [\"bill_length_mm\", \"bill_depth_mm\"], then the rollup specification should be:\n{\n  bill_length_mm: {expr: `(d) => op.min(d[\"bill_length_mm\"])`, func: true },\n  bill_depth_mm: {expr: `(d) => op.min(d[\"bill_depth_mm\"])`, func: true }\n}\nUsing the object above and example, here’s how we describe rollup (aggregation) operations:\n\nThe object’s names are column names in the resulting table.\nThe object’s values are expressed as functions.\n\nthe function takes the “data frame” as an argument; you can subset the data frame by column-name.\nfor security reasons, by default, arquero makes only certain operations available by default; these operations are contained in the op object.\n\n\nWe can build the rollup object by using a reduce() function on the cols_group array:\n\nThe accumulator is initalized with an empty object, {}.\nFor each value ,val, in the cols_group array, given the accumulator, acc:\n\nreturn a new object containing acc and a new named element.\n\n\nIt can be a lot to absorb JavaScript, functional programming, and the peculiarities of arquero all at once. Keep in mind that you can apply the functional programming you learned using purrr, and your knowledge of how group_by() and summarise() work in dplyr.\nHere’s the equivalent in R, using purrr and rlang:\nreducer <- function(acc, val, func) {\n\n  mapped <- \n    rlang::list2(\n      \"{val}\" := list(\n        expr = glue::glue('(d) => op.{func}(d[\"{val}\"])'), \n        func = TRUE\n      )\n    )\n\n  c(acc, mapped)\n}\n\nvalues <- purrr::reduce(cols_agg, reducer, func = func_agg, .init = list())\nThis gets heavy because we have to use rlang::list2() to interpolate the names: \"{val}\" :=.\nWe don’t have the same check here to validate the aggregation function. Security considerations are a little bit different when using Observable. Because Observable runs this app entirely in the user’s browser, there is no server component. Thus, the user is free to run whatever code they like - it’s a bit like an IDE in that respect.\nThere are some considerations around protecting secrets, but these do not apply to this app.\n\n\n3.2.3 Showcase\nHere’s where we show what the notebook can do. First, we display the inp table, using Observable’s built-in Inputs.Table():\nviewof table_inp = Inputs.table(inp)\ntable_inp has a value (we can select rows), but we don’t use it.\nNext, we have an input for the grouping columns. We are using the columnNamesPredicate() function using Lodash’s _.isString:\nviewof cols_group = Inputs.select(columnNamesPredicate(inp, _.isString), {\n  label: \"Grouping columns\",\n  multiple: true\n})\nThe input for cols_agg is almost identical; there, we use _.isNumber as a predicate.\nThe input for func_agg is fairly straightforward:\nviewof func_agg = Inputs.select([\"mean\", \"min\", \"max\"], {\n  label: \"Aggregation function\",\n  multiple: false\n})\nFor each of these inputs: cols_group, cols_agg, and func_agg, the value is the selection.\nThe button is less straightforward; we view is the button, but the value is the aggregated table. The two are joined by a reduce option, a function that is run whenever the button is clicked.\nIn our case, the reduce function runs the query on the inp table, and returns the aggregated table.\nviewof agg = Inputs.button(\"Submit\", {\n  value: aq.table(),\n  reduce: () => {\n    return aq\n      .queryFrom(buildQueryObject(cols_group, cols_agg, func_agg))\n      .evaluate(inp);\n  }\n})\nFinally, we display the agg table:\nviewof table_agg = Inputs.table(agg)"
  },
  {
    "objectID": "field-guide-python.html",
    "href": "field-guide-python.html",
    "title": "Appendix A — Field Guide to Python",
    "section": "",
    "text": "In this appendix, we focus on how to get up-and-running in Python, and how you can make your Python environment reprodicible. Once you have established this, RStudio have a useful guide for how to “think” in Python, knowing R."
  },
  {
    "objectID": "field-guide-python.html#python-installation",
    "href": "field-guide-python.html#python-installation",
    "title": "Appendix A — Field Guide to Python",
    "section": "A.1 Python installation",
    "text": "A.1 Python installation\nThe first order of business is to make sure that you have a recent version of Python installed. By recent, I mean one of the last two minor version; as of February 2022, these are versions 3.10 and 3.9.\nTo check your default Python version, just type python -V (note the captial) at the terminal command line (not the R command line). This is what I see:\n> python -V\nPython 3.10.2\nThere is a variety of strategies for managing Python on your computer, perhaps the simplest is to go to the Python downloads page, then go from there."
  },
  {
    "objectID": "field-guide-python.html#project-management",
    "href": "field-guide-python.html#project-management",
    "title": "Appendix A — Field Guide to Python",
    "section": "A.2 Project management",
    "text": "A.2 Project management\n\nA.2.1 Git\nIf you are creating your project directory from scratch, you will likely want to initialize a git repository in your newly-created directory:\n> git init\nYou will also want a .gitignore file for your project. Here’s my .gitignore file for the Dash demo app; I include some RStudio stuff for if/when I open the project in RStudio.\n# RStudio stuff\n.Rproj.user\n*.Rproj\n.Rhistory\n.Rdata\n\n# virtual environment\nvenv\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\nYou may wish to adapt the virtual-environment entry to your situation.\n\n\nA.2.2 Virtual Environment\nPython virtual environments are used to manage dependencies so that they are local to a project. This is a different from the classic R idea of having a single library of packages used for all projects.\nThe idea of a project-based is also used in JavaScript (e.g. npm, yarn) and is gaining popularity in R with the renv package. In fact, this book is build using renv.\nThe goal here is to show you how to establish and manage a virtual environment for a Dash project.\nIn your newly-created project directory, from the terminal command-line:\n> python -m venv ./venv\nThis creates your Python virtual environment by creating a directory in the root of your project called venv. The name of the directory is determined by the last argument, in this case ./venv. There are a number of “standard” ways to name virtual environments; \"venv\" is one of them. It’s really up to you and your collaborators.\nThe important thing is to make sure that you have a .gitignore entry for the virtual-environment directory.\nNext, let’s activate the environment. This tells your terminal that this is what you want to run when you invoke python.\nIn your project directory, from the terminal command-line:\n> source ./venv/bin/activate\nAt this point, you might want to install packages into your virtual enviromment. Which packages will depend on the particulars of your project, but you can start with Dash:\n> pip install dash\nEvery so often, you will want to catpure which packages have been installed into your virtual environment:\n> pip freeze > requirements.txt\nYou will want to commit requirements.txt to your git repository, as this contains the instructions for someone to reproduce your virtual environment.\nTo reproduce it, a colleague (perhaps you!) will have to create and activate a virtual environment, then:\n> pip install -r requirements.txt\nThese are the very basics for how to set up and maintain a Python project. As you gain experience, you will likely adpat these ideas to your evolving needs."
  },
  {
    "objectID": "deployment.html",
    "href": "deployment.html",
    "title": "Appendix B — Deployment",
    "section": "",
    "text": "In time, I will reconstruct from my code (and my notes) the tricks and sorcery I used to deploy these apps."
  },
  {
    "objectID": "deployment.html#shiny",
    "href": "deployment.html#shiny",
    "title": "Appendix B — Deployment",
    "section": "B.1 Shiny",
    "text": "B.1 Shiny\nNot a lot of trickery here; I got an shinyapps account and I used the deployment service built into the RStudio IDE."
  },
  {
    "objectID": "deployment.html#dash",
    "href": "deployment.html#dash",
    "title": "Appendix B — Deployment",
    "section": "B.2 Dash",
    "text": "B.2 Dash\nSome trickery here to deploy to Heroku."
  },
  {
    "objectID": "deployment.html#observable",
    "href": "deployment.html#observable",
    "title": "Appendix B — Deployment",
    "section": "B.3 Observable",
    "text": "B.3 Observable\nThe code is developed in a “deployed” state; I hit the “Publish” button.\nThat said, there are some interesting embedding possibilities and, as of Spring 2021, capability to run interactively via Quarto."
  },
  {
    "objectID": "stray-thoughts.html",
    "href": "stray-thoughts.html",
    "title": "Appendix C — Stray Thoughts",
    "section": "",
    "text": "Shiny and Observable are more opinionated than Dash.\nShiny and Observable manage the reactive graph for you; Dash makes you spell it out explicitly.\nBeing more opinionated lets you code more concisely. Being less opinionated gives you more flexibility in what you create, at the price of more code.\nShiny lets you store the state at the server; Dash and Observable do not.\nDash and Observable use React under the hood.\nShiny and Dash both offer a UI/server framework; in Observable, it’s all notebook.\nObservable supports (and encourages) the idea of importing functions from other published notebooks."
  }
]